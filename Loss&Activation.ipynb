{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENT ACTIVATION FUNCTIONS WITH PYTHON AND NUMPY"
      ],
      "metadata": {
        "id": "AtL9Jmt9ZrrO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX5piNGhq45X",
        "outputId": "cc909ec9-98eb-4fe1-f94c-6fe69bc8001d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-8.29415404  2.58126219 -1.63519635]\n",
            "  [ 1.18195215  3.88352743 -7.44073997]\n",
            "  [ 8.22183407  2.96481674 -5.59173037]]\n",
            "\n",
            " [[ 9.78099613 -3.52056322 -8.49343998]\n",
            "  [-2.14733803  1.71114413 -9.66659434]\n",
            "  [-6.19494784 -7.16000057  8.12186391]]\n",
            "\n",
            " [[-9.15044385  2.38678294  3.10308447]\n",
            "  [ 5.64454996  9.26244545 -7.00962563]\n",
            "  [-9.10744832  6.78037136 -5.75521856]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "vec = np.random.uniform(-10, 10, 27).reshape(3,3,3)\n",
        "print(vec)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class activation:\n",
        "  def __init__(self, vec, axis=0):\n",
        "    self.vec = vec\n",
        "    self.axis = axis\n",
        "\n",
        "  def relu(self):\n",
        "    return np.maximum(self.vec, 0)\n",
        "\n",
        "  def sigmoid(self, hard=False):\n",
        "    if hard:\n",
        "      return np.where(1/(1+np.exp(-self.vec)) >= 0.5, 1, 0)\n",
        "    return 1/(1+np.exp(-self.vec))\n",
        "\n",
        "  def softmax(self,stable=True):\n",
        "    if stable:\n",
        "      self.vec -= np.max(self.vec, axis=self.axis, keepdims=True)\n",
        "    return np.exp(self.vec)/np.sum(np.exp(self.vec), axis = self.axis, keepdims=True)\n",
        "\n",
        "  def tanh(self,stable=True):\n",
        "    if stable:\n",
        "      self.vec -= np.max(self.vec, axis=self.axis, keepdims=True)\n",
        "    return np.exp(self.vec) - np.exp(-self.vec)/np.exp(self.vec) + np.exp(-self.vec)\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.vec)"
      ],
      "metadata": {
        "id": "mY6_nHGyreHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# arr = np.array([[-1, 2, -3], [4, -5, 6]])\n",
        "# vec = np.array([[0.1,0.3,0.6],[0.3,0.2,0.5],[0.8,0.1,0.1]])\n",
        "act = activation(vec)\n",
        "#result = act.relu()\n",
        "#result = act.sigmoid(True)\n",
        "result = act.softmax()\n",
        "# result = act.softmax(False)\n",
        "#result = act.tanh(False)\n",
        "# result = act.tanh()\n",
        "#print(act)\n",
        "print(result)\n",
        "print(np.sum(result, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Xn4y2UZGtC5W",
        "outputId": "54c18712-1fb5-4cd5-bb27-503b2418e510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vec' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8e035e362e06>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# arr = np.array([[-1, 2, -3], [4, -5, 6]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# vec = np.array([[0.1,0.3,0.6],[0.3,0.2,0.5],[0.8,0.1,0.1]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#result = act.relu()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#result = act.sigmoid(True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vec' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENT BASIC LOSS FUNCTIONS WITH PYTHON AND NUMPY"
      ],
      "metadata": {
        "id": "TMnj8wFLZ6B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class loss:\n",
        "  def __init__(self, y_true, y_pred, axis=0):\n",
        "    self.y_true = y_true\n",
        "    self.y_pred = y_pred\n",
        "    self.axis = axis\n",
        "    self.activation = activation(y_pred, axis)\n",
        "\n",
        "  def mse(self):\n",
        "    return np.mean(np.power(self.y_true - self.y_pred, 2))\n",
        "\n",
        "  def mae(self):\n",
        "    return np.mean(np.abs(self.y_true - self.y_pred))\n",
        "\n",
        "  def binary_cross_entropy(self):\n",
        "    return -np.sum(self.y_true * np.log(self.y_pred) + (1 - self.y_true) * np.log(1 - self.y_pred))\n",
        "\n",
        "  def categorical_cross_entropy(self, axis = 0, epsilon=1e-15):\n",
        "    self.y_pred = self.activation.softmax()\n",
        "    self.y_pred = np.clip(self.y_pred, epsilon, 1-epsilon)\n",
        "    return -np.sum(self.y_true * np.log(self.y_pred), axis=axis)\n",
        "\n",
        "  def categorical_cross_entropy_with_class_labels(self, epsilon=1e-15):\n",
        "    self.y_pred = self.activation.softmax()\n",
        "    self.y_pred = np.clip(self.y_pred, epsilon, 1-epsilon)\n",
        "\n",
        "    self.batch_index = np.arange(self.y_true.shape[0])\n",
        "    self.y_pred = self.y_pred[self.batch_index, self.y_true]\n",
        "    print(self.y_pred)\n",
        "\n",
        "    return -np.sum(np.log(self.y_pred))\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.y_true) + \" \" + str(self.y_pred)\n"
      ],
      "metadata": {
        "id": "5Lwtarjot0bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss1 = loss(np.array([1,2,3]), np.array([4,5,6]))\n",
        "# print(Loss1.mse())\n",
        "# print(Loss1.mae())\n",
        "# Loss2 = loss(np.array([1,0,1,0]), np.array([0.7,0.2,0.1,0.1]))\n",
        "# print(Loss2.binary_cross_entropy())\n",
        "target = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
        "target_labels = np.argmax(target, axis=1)\n",
        "#print(target_labels)\n",
        "logits = np.array([[0.1,0.3,0.6],[0.3,0.2,0.5],[0.8,0.1,0.1]])\n",
        "Loss3 = loss(target, logits, axis=1)\n",
        "print(Loss3.categorical_cross_entropy(None)) # sum over class\n",
        "print(Loss3.categorical_cross_entropy())\n",
        "# Loss4 = loss(target_labels, logits)\n",
        "# print(Loss4.categorical_cross_entropy_with_class_labels()) # sum over batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om5GNmikzWUN",
        "outputId": "46d658c4-d7a5-4aac-a833-f6d9d9821f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.9828442571439258\n",
            "[1.35328656 1.23983106 1.38972664]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENT SIMCLR LOSS FUNCTION WITH CROSS ENTROPY"
      ],
      "metadata": {
        "id": "srra53eJaAa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NT_Xent:\n",
        "  def __init__(self, z, temperature=0.5):\n",
        "    self.feat = z\n",
        "    self.temp = temperature\n",
        "\n",
        "  def forward(self):\n",
        "    B,V,D = self.feat.shape\n",
        "    assert V == 2\n",
        "    self.feat = self.feat.reshape(B*V, D)\n",
        "\n",
        "    self.feat = self.feat/ np.linalg.norm(self.feat, axis=1, keepdims=True)\n",
        "    self.sim = np.dot(self.feat, self.feat.T)/self.temp\n",
        "\n",
        "    #np.fill_diagonal(self.sim, -np.inf)\n",
        "\n",
        "    mask = np.eye(2*B)\n",
        "    self.sim[mask==1] = -np.inf\n",
        "\n",
        "    target = [i+1 if i%2==0 else i-1 for i in range(2*B)]\n",
        "\n",
        "    logit = self.sim - np.max(self.sim, axis=1, keepdims=True)\n",
        "    logit_exp_sum = np.sum(np.exp(logit), axis = 1, keepdims=True)\n",
        "    logit_prob = logit - np.log(logit_exp_sum)\n",
        "\n",
        "    return -np.mean(logit_prob[np.arange(2*B), target])\n",
        "\n",
        "vec = np.random.uniform(-10, 10, 50).reshape(5,2,5)\n",
        "ContaLoss = NT_Xent(vec)\n",
        "print(ContaLoss.forward())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7LYG0ak0FtP",
        "outputId": "42b235e0-835c-44b7-d4c7-289e88f99db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.585308080309623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENT SIMCLR LOSS FUNCTION WITHOUT CROSS ENTROPY"
      ],
      "metadata": {
        "id": "WgxKIhxiazbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def simclr_contrastive_loss(z, temperature=0.5):\n",
        "    \"\"\"\n",
        "    Computes the SimCLR contrastive loss (NT-Xent Loss) for a batch of embeddings.\n",
        "\n",
        "    Args:\n",
        "        z (numpy.ndarray): Embeddings of shape (2N, D) where N is the batch size\n",
        "                           and D is the embedding dimension. Should be L2 normalized.\n",
        "        temperature (float): Temperature parameter.\n",
        "\n",
        "    Returns:\n",
        "        float: Scalar loss value.\n",
        "    \"\"\"\n",
        "    # Step 1: Normalize embeddings\n",
        "    z = z / np.linalg.norm(z, axis=1, keepdims=True)\n",
        "\n",
        "    # Step 2: Compute similarity matrix\n",
        "    sim_matrix = np.dot(z, z.T)  # Shape: (2N, 2N)\n",
        "\n",
        "    # Step 3: Apply temperature scaling\n",
        "    sim_matrix /= temperature\n",
        "\n",
        "    # Step 4: Mask out self-similarities (diagonal)\n",
        "    batch_size = z.shape[0]\n",
        "    mask = np.eye(batch_size, dtype=bool)\n",
        "    sim_matrix_no_diag = np.where(mask, -np.inf, sim_matrix)  # logsumexp-safe\n",
        "\n",
        "    # Step 5: For each positive pair (i,j)\n",
        "    # We assume: first N are view 1, second N are view 2, so positive pairs are (i, i+N) and (i+N, i)\n",
        "    N = batch_size // 2\n",
        "    positives = np.concatenate([\n",
        "        np.arange(N)[:, None],            # (0,1,2,...N-1)\n",
        "        np.arange(N, 2*N)[:, None]        # (N,N+1,...2N-1)\n",
        "    ], axis=1)\n",
        "\n",
        "    loss = 0.0\n",
        "    sim_matrix_no_diag -= np.max(sim_matrix_no_diag, axis=1, keepdims=True)\n",
        "    for i, j in positives:\n",
        "        # numerator: exp(sim(i, j))\n",
        "        numerator = np.exp(sim_matrix_no_diag[i, j])\n",
        "        # denominator: sum over all except itself\n",
        "        denominator = np.sum(np.exp(sim_matrix_no_diag[i, :]))\n",
        "        loss += -np.log(numerator / denominator)\n",
        "\n",
        "    # Average loss\n",
        "    loss /= (2 * N)\n",
        "    return loss\n",
        "#vec = np.random.uniform(-10, 10, 50).reshape(5,2,5)\n",
        "new_vec = np.concatenate((vec[:,0,:], vec[:,1,:]), axis=0)\n",
        "ContaLoss = simclr_contrastive_loss(new_vec)\n",
        "print(ContaLoss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjRRqPxb0cgC",
        "outputId": "6f5b1d58-fc79-485c-8253-057dcf9eb852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2159902266494427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENT TRIPLET LOSS WITH PYTHON AND NUMPY"
      ],
      "metadata": {
        "id": "kS-TfObfa5Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class tripletLoss:\n",
        "  def __init__(self, anc, pos, neg, margin=1.0):\n",
        "    self.anc = anc\n",
        "    self.pos = pos\n",
        "    self.neg = neg\n",
        "    self.margin = margin\n",
        "\n",
        "  def forward(self):\n",
        "    anc = self.anc/np.linalg.norm(self.anc, axis=1, keepdims=True)\n",
        "    pos = self.pos/np.linalg.norm(self.pos, axis=1, keepdims=True)\n",
        "    neg = self.neg/np.linalg.norm(self.neg, axis=1, keepdims=True)\n",
        "\n",
        "    pos_dist = np.sum(np.square(anc - pos), axis=1)\n",
        "    neg_dist = np.sum(np.square(anc - neg), axis=1)\n",
        "\n",
        "    return np.mean(np.maximum(pos_dist - neg_dist + self.margin, 0))\n",
        "\n",
        "anc = np.random.uniform(-10, 10, 100).reshape(10,10)\n",
        "pos  = np.random.uniform(-10, 10, 100).reshape(10,10)\n",
        "neg  = np.random.uniform(-10, 10, 100).reshape(10,10)\n",
        "TripLoss = tripletLoss(anc, pos, neg)\n",
        "print(TripLoss.forward())"
      ],
      "metadata": {
        "id": "VeEXcp6j0qfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d75fe9-afa0-4f5c-b916-489e9b5a5181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0662874818049741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DINO LOSS FOR SELF-DISTILATION"
      ],
      "metadata": {
        "id": "_0iKGbFubFE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DINOLossNumpy:\n",
        "    def __init__(self, out_dim, ncrops, warmup_teacher_temp, teacher_temp,\n",
        "                 warmup_teacher_temp_epochs, nepochs, student_temp=0.1,\n",
        "                 center_momentum=0.9):\n",
        "        self.student_temp = student_temp\n",
        "        self.center_momentum = center_momentum\n",
        "        self.ncrops = ncrops\n",
        "        self.center = np.zeros((1, out_dim))  # shape (1, out_dim)\n",
        "\n",
        "        # Create teacher temperature schedule\n",
        "        self.teacher_temp_schedule = np.concatenate((\n",
        "            np.linspace(warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs),\n",
        "            np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp\n",
        "        ))\n",
        "\n",
        "    def softmax(self, x, axis=-1):\n",
        "        x = x - np.max(x, axis=axis, keepdims=True)  # for numerical stability\n",
        "        exp_x = np.exp(x)\n",
        "        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "\n",
        "    def log_softmax(self, x, axis=-1):\n",
        "        x = x - np.max(x, axis=axis, keepdims=True)  # for numerical stability\n",
        "        logsumexp = np.log(np.sum(np.exp(x), axis=axis, keepdims=True))\n",
        "        return x - logsumexp\n",
        "\n",
        "    def forward(self, student_output, teacher_output, epoch):\n",
        "        \"\"\"\n",
        "        student_output: shape [B * ncrops, out_dim]\n",
        "        teacher_output: shape [B, out_dim]\n",
        "        \"\"\"\n",
        "        B = teacher_output.shape[0]\n",
        "        student_out = student_output / self.student_temp\n",
        "        student_out = np.array_split(student_out, self.ncrops, axis=0)  # tuple of [B, out_dim]\n",
        "        #print('Student Out:', len(student_out), student_out[0])\n",
        "\n",
        "        # teacher centering and sharpening\n",
        "        temp = self.teacher_temp_schedule[epoch]\n",
        "        teacher_out = self.softmax((teacher_output - self.center) / temp, axis=-1)\n",
        "        teacher_out = np.array_split(teacher_out, 2, axis=0)  # assuming teacher uses 2 global crops\n",
        "\n",
        "        total_loss = 0.0\n",
        "        n_loss_terms = 0\n",
        "\n",
        "        for iq, q in enumerate(teacher_out):\n",
        "            for v in range(len(student_out)):\n",
        "                if v == iq:\n",
        "                    continue\n",
        "                # Cross-entropy between teacher and student outputs\n",
        "                log_probs = self.log_softmax(student_out[v], axis=-1)\n",
        "                loss = -np.sum(q * log_probs, axis=-1)  # shape: [B]\n",
        "                total_loss += np.mean(loss)\n",
        "                n_loss_terms += 1\n",
        "\n",
        "        total_loss /= n_loss_terms\n",
        "\n",
        "        self.update_center(teacher_output)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def update_center(self, teacher_output):\n",
        "        \"\"\"\n",
        "        Exponential moving average update of the center.\n",
        "        teacher_output: shape [2B, out_dim]\n",
        "        \"\"\"\n",
        "        batch_center = np.mean(teacher_output, axis=0, keepdims=True)\n",
        "        #print('batch_center: ',batch_center.shape, batch_center)\n",
        "        self.center = self.center * self.center_momentum + \\\n",
        "                      batch_center * (1 - self.center_momentum)\n",
        "\n"
      ],
      "metadata": {
        "id": "AB9S5O5Lgb2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "B, out_dim = 4, 10\n",
        "ncrops = 6\n",
        "\n",
        "student_output = np.random.randn(B * ncrops, out_dim)\n",
        "teacher_output = np.random.randn(B * 2, out_dim)\n",
        "epoch = 5\n",
        "\n",
        "dino_loss = DINOLossNumpy(\n",
        "    out_dim=out_dim,\n",
        "    ncrops=ncrops,\n",
        "    warmup_teacher_temp=0.04,\n",
        "    teacher_temp=0.07,\n",
        "    warmup_teacher_temp_epochs=10,\n",
        "    nepochs=100\n",
        ")\n",
        "\n",
        "loss = dino_loss.forward(student_output, teacher_output, epoch)\n",
        "print(\"Loss:\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3HHnPbinwzi",
        "outputId": "ef5440db-c4f6-48b4-b8d3-7bf08abf289d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Out: 6 [[ 17.64052346   4.00157208   9.78737984  22.40893199  18.6755799\n",
            "   -9.7727788    9.50088418  -1.51357208  -1.03218852   4.10598502]\n",
            " [  1.44043571  14.54273507   7.61037725   1.21675016   4.43863233\n",
            "    3.33674327  14.94079073  -2.05158264   3.13067702  -8.54095739]\n",
            " [-25.52989816   6.53618595   8.64436199  -7.4216502   22.69754624\n",
            "  -14.54365675   0.45758517  -1.8718385   15.32779214  14.6935877 ]\n",
            " [  1.54947426   3.7816252   -8.87785748 -19.80796468  -3.47912149\n",
            "    1.56348969  12.30290681  12.02379849  -3.87326817  -3.02302751]]\n",
            "batch_center:  (1, 10) [[-0.38563021 -0.1221811   0.21694348 -0.151761   -0.22178317  0.02449219\n",
            "  -0.10994892 -0.02136662 -0.47691968  0.33885894]]\n",
            "Loss: 15.40925833750359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DinoLoss:\n",
        "  def __init__(self, ncrops, embed_dim, teach_temp, teach_temp_warm, teach_temp_epochs, nepochs, momentum=0.9,\n",
        "               stud_temp=0.1):\n",
        "    self.ncrops = ncrops # number of image crops\n",
        "    self.embed_dim = embed_dim #feature dimension from vit\n",
        "    self.tea_temp = teach_temp # temprature of teacher\n",
        "    self.tea_temp_warm = teach_temp_warm # minimum temprature of teacher\n",
        "    self.tea_temp_epochs = teach_temp_epochs # num epoch to schedule techer temprature\n",
        "    self.nepo = nepochs # total number of training epochs\n",
        "    self.mom = momentum #momentum to maintain moving average of teacher crops mean\n",
        "    self.stud_temp = stud_temp # temprature of student\n",
        "\n",
        "    self.center = np.zeros((1, embed_dim)) # center for teacher output of size [2B, embed_dim]\n",
        "    self.tea_temp_schedule = np.concatenate((\n",
        "        np.linspace(teach_temp_warm, teach_temp, teach_temp_epochs),\n",
        "        np.ones(nepochs-teach_temp_epochs)*teach_temp)) #schedule teacher temprature for epoches\n",
        "\n",
        "  def softmax(self, x, axis=-1):\n",
        "      x = x - np.max(x, axis=axis, keepdims=True)  # for numerical stability\n",
        "      exp_x = np.exp(x)\n",
        "      return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "\n",
        "  def log_softmax(self, x, axis=-1):\n",
        "      x = x - np.max(x, axis=axis, keepdims=True)  # for numerical stability\n",
        "      logsumexp = np.log(np.sum(np.exp(x), axis=axis, keepdims=True))\n",
        "      return x - logsumexp\n",
        "\n",
        "  def forward(self, stud_out, tech_out, epoch):\n",
        "      '''\n",
        "      studen_out shape [B*ncrops, embed_dim]\n",
        "      teacher_out shape [B, embed_dim]\n",
        "      '''\n",
        "      # Process student output\n",
        "      stud_out = stud_out/self.stud_temp\n",
        "      stud_out = np.array_split(stud_out, self.ncrops, axis=0) # list of ncrops of size [B, embed_dim]\n",
        "\n",
        "      #process teacher outputs\n",
        "      temp = self.tea_temp_schedule[epoch]\n",
        "      tech_out = self.softmax((tech_out-self.center)/temp, axis=-1)\n",
        "      tech_out = np.array_split(tech_out, 2, axis=0) # list of 2 crops of size [B, embed_dim]\n",
        "\n",
        "      total_loss = 0.0\n",
        "      num_loss = 0\n",
        "\n",
        "\n",
        "      for iq, q in enumerate(tech_out): # iterate over two crops of teacher\n",
        "        for v in range(len(stud_out)): # iterate iver n crops of student\n",
        "          if v == iq: # skip self similarity\n",
        "            continue\n",
        "\n",
        "          log_prob = self.log_softmax(stud_out[v], axis=-1) # compute log(softmax(stud_out[v]))\n",
        "          loss = -np.sum(q*log_prob, axis=-1) # compute cross_enropy (-softmax(tech_out) * log(softmax(stud_out[v])))\n",
        "          total_loss += np.mean(loss) # compute mean of batch\n",
        "          num_loss += 1\n",
        "\n",
        "      total_loss /= num_loss\n",
        "\n",
        "      self.update_center(tech_out)\n",
        "\n",
        "      return total_loss\n",
        "\n",
        "  def update_center(self, tech_out):\n",
        "      '''\n",
        "      tech_out shape [2B, embed_dim]\n",
        "      '''\n",
        "      batch_center = np.mean(tech_out, axis=0, keepdims=True)\n",
        "      self.center = self.center*self.mom + batch_center*(1-self.mom)\n"
      ],
      "metadata": {
        "id": "wv9ga0bjqBiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "B, out_dim = 4, 10\n",
        "ncrops = 6\n",
        "\n",
        "student_output = np.random.randn(B * ncrops, out_dim)\n",
        "teacher_output = np.random.randn(B * 2, out_dim)\n",
        "epoch = 5\n",
        "\n",
        "dino_loss = DinoLoss(\n",
        "    embed_dim=out_dim,\n",
        "    ncrops=ncrops,\n",
        "    teach_temp_warm=0.04,\n",
        "    teach_temp=0.07,\n",
        "    teach_temp_epochs=10,\n",
        "    nepochs=100\n",
        ")\n",
        "\n",
        "loss = dino_loss.forward(student_output, teacher_output, epoch)\n",
        "print(\"Loss:\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpoRphnf3m1k",
        "outputId": "0e6d3780-340e-4101-b152-d227959755a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 15.40925833750359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum(target*logits, axis=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyMQnlj83vFh",
        "outputId": "b2b20da4-d3a6-4022-c789-0d1d288c0279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.5 -0.3 -0.7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4PmmWp54gMB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}