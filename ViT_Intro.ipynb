{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: syn with drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nri3GeYHuxwk",
        "outputId": "012df211-034c-4e00-9ad0-468ea18a10be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rulRViqtPYr",
        "outputId": "7d2c01b4-3e81-4971-b4fb-dee0f3fd41a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "#extract patch of given size from an image\n",
        "import torch\n",
        "\n",
        "def extraxt_patch(img, patch_size):\n",
        "  c, h, w = img.shape\n",
        "  assert h % patch_size == 0 and w % patch_size == 0\n",
        "  patches = []\n",
        "  for i in range(0, h, patch_size):\n",
        "    for j in range(0, w, patch_size):\n",
        "      patch = img[:, i:i+patch_size, j:j+patch_size]\n",
        "      patches.append(patch)\n",
        "  patches = torch.stack(patches)\n",
        "  return patches\n",
        "\n",
        "img = torch.randn(3,32,32)\n",
        "patch_size = 8\n",
        "patches = extraxt_patch(img, patch_size)\n",
        "print(patches.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image from non-overlapping patch\n",
        "def patch2image(patches, patch_size, img_size=32, num_channel = 3):\n",
        "  N, C, H, W = patches.shape\n",
        "  print(patches.shape)\n",
        "\n",
        "  patches = patches.view(img_size//patch_size, img_size//patch_size, num_channel, patch_size, patch_size)\n",
        "  print(patches.shape)\n",
        "\n",
        "  patches = patches.permute(2, 0, 3, 1, 4)\n",
        "  print(patches.shape)\n",
        "\n",
        "  img = patches.contiguous().view(num_channel, img_size, img_size)\n",
        "  print(img.shape)\n",
        "  return img\n",
        "\n",
        "\n",
        "img = patch2image(patches, patch_size)\n",
        "print(img.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9Q7PYJSdkKZ",
        "outputId": "d40314e3-813e-4bcf-9ebc-207d213592d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 8, 8])\n",
            "torch.Size([4, 4, 3, 8, 8])\n",
            "torch.Size([3, 4, 8, 4, 8])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# design a function to generate linear embeddings of patches\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class LinearEmbeddings(nn.Module):\n",
        "  def __init__(self, img_size = 224, patch_size=8, embed_dim=768):\n",
        "    super().__init__()\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_patches = (img_size // patch_size) ** 2\n",
        "    self.LinearProjection = nn.Linear(patch_size * patch_size * 3, embed_dim)\n",
        "\n",
        "  def extract_patches(self, x):\n",
        "    patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "    patches = patches.reshape(x.shape[0], -1, self.patch_size * self.patch_size * 3)\n",
        "    return patches\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, c, h, w = x.shape\n",
        "    assert h == self.img_size and w == self.img_size, f\"Input image size ({h}*{w}) doesn't match model ({self.img_size}*{self.img_size})\"\n",
        "    patches = self.extract_patches(x)\n",
        "    embeddings = self.LinearProjection(patches)\n",
        "    return embeddings\n",
        "\n",
        "LE = LinearEmbeddings()\n",
        "x = torch.randn(2, 3, 224, 224)\n",
        "embeddings = LE(x)\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uOaSEMJaq25",
        "outputId": "e4f23475-1051-45f6-95f2-9c942fed2d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 784, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: import image '/content/drive/MyDrive/GraPix POSTER.png' and resize and do the same\n",
        "# import torch\n",
        "# from PIL import Image\n",
        "# from torchvision import transforms\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def extract_patch(img, patch_size):\n",
        "#   patches = []\n",
        "#   c,h,w = img.shape\n",
        "#   print('Image Shape: ',img.shape)\n",
        "#   for i in range(0, h, patch_size):\n",
        "#     for j in range(0, w, patch_size):\n",
        "#       patch = img[:, i:i+patch_size, j:j+patch_size]\n",
        "#       patches.append(patch)\n",
        "#   patches = torch.stack(patches)\n",
        "#   return patches\n",
        "\n",
        "\n",
        "# img_path = '/content/drive/MyDrive/GraPix POSTER.png'\n",
        "# img = Image.open(img_path).convert('RGB')\n",
        "# # # Resize the image (example: resize to 256x256)\n",
        "# # new_size = (256, 256)\n",
        "# # resized_img = img.resize(new_size)\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((256,256)),\n",
        "#     transforms.ToTensor()\n",
        "# ])\n",
        "# resized_img = transform(img)\n",
        "# patch_size = 64\n",
        "# patches = extract_patch(resized_img, patch_size)\n",
        "# print(patches.shape)\n",
        "\n",
        "# #plot patches\n",
        "# # fig, axs = plt.subplots(patches.shape[0], 1, figsize=(10, 10))\n",
        "# # for i in range(patches.shape[0]):\n",
        "# #   axs[i].imshow(patches[i].permute(1,2,0))\n",
        "# #   axs[i].axis('off')\n",
        "# #plt.show()"
      ],
      "metadata": {
        "id": "iA8mZleFvVcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# design a function for assigning positional embedding to patch embeddings\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEmbeddings(nn.Module):\n",
        "  def __init__(self, seq_len, dim):\n",
        "    super().__init__()\n",
        "    self.pos_embeddings = nn.Parameter(torch.randn(1, seq_len, dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x + self.pos_embeddings\n",
        "\n",
        "seq_len = 10\n",
        "dim = 128\n",
        "pe = PositionalEmbeddings(seq_len, dim)\n",
        "x = torch.randn(1, seq_len, dim)\n",
        "y = pe(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDfrTUFX0sdr",
        "outputId": "cbd91f40-9a52-4d35-bb3c-c70620f69411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sinusoidal posotional embeddings for patch tokens\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class SinusoidalPositionalEmbeddings(nn.Module):\n",
        "    def __init__(self, seq_len, dim):\n",
        "        super().__init__()\n",
        "        self.register_buffer('pos_embeddings', self._build_positional_encoding(seq_len, dim), persistent=False)\n",
        "\n",
        "    def _build_positional_encoding(self, seq_len, dim):\n",
        "        pe = torch.zeros(seq_len, dim)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))  # (dim/2)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
        "        return pe.unsqueeze(0)  # shape (1, seq_len, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embeddings[:, :x.size(1), :]\n",
        "\n",
        "seq_len = 10\n",
        "dim = 128\n",
        "spe = SinusoidalPositionalEmbeddings(seq_len, dim)\n",
        "x = torch.randn(1, seq_len, dim)\n",
        "y = spe(x)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "pprkf2d7keEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# design multi-head attention for ViT\n",
        "import torch.nn as nn\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.attention = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.attention(x,x,x)[0]\n",
        "\n",
        "attention = MultiheadAttention(dim=128, num_heads=8)\n",
        "x = torch.randn(1, 10, 128)\n",
        "y = attention(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz9y7a1g1ypE",
        "outputId": "fdb54ebd-017a-4ce0-873b-9691068af97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# design multi-head attention with layer normalization for ViT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiheadAttentionNormDropout(nn.Module):\n",
        "  def __init__(self, dim, num_heads, dropout):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(dim)\n",
        "    self.MHA = nn.MultiheadAttention(embed_dim = dim, num_heads=num_heads, batch_first=True)\n",
        "    self.Dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.norm(x)\n",
        "    x = self.MHA(x,x,x)[0]\n",
        "    x = self.Dropout(x)\n",
        "    return x\n",
        "\n",
        "MHAND = MultiheadAttentionNormDropout(dim=128, num_heads=8, dropout=0.1)\n",
        "x = torch.randn(1, 10, 128)\n",
        "y = MHAND(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecHFkladl8js",
        "outputId": "7377cfb8-77fb-4f50-a40f-0c5c8149a9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# design transformer encoder block\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self, dim, num_heads, mlp_ratio):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim)\n",
        "    self.atten = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(dim, int(dim * mlp_ratio)),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(int(dim * mlp_ratio), dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.atten(self.norm1(x),self.norm1(x),self.norm1(x))[0]\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x\n",
        "\n",
        "TEB = TransformerEncoderBlock(dim=128, num_heads=8, mlp_ratio=4)\n",
        "x = torch.randn(1, 10, 128)\n",
        "y = TEB(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRk8zRmB4VvX",
        "outputId": "81b8469b-c8e9-4f1c-ab4d-66de8f7996dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# design function for post processing of image before sending to ViT encoder\n",
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, img_size = 224, in_channels = 3, embed_dim = 768, patch_size =16):\n",
        "    super().__init__()\n",
        "    self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "    self.pos_embeddings = nn.Parameter(torch.randn(1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
        "    #self.pos_embeddings = nn.sin\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    x = self.projection(x)\n",
        "    x = x.flatten(2)\n",
        "    x = x.transpose(1,2)\n",
        "    cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "    x = x + self.pos_embeddings\n",
        "    return x\n",
        "\n",
        "PE = PatchEmbedding()\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "y = PE(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H66oRT58c09",
        "outputId": "66ee37e1-7ba9-46ec-a5ec-ef994ff9d233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement vanilla ViT for classification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, img_size = 224, in_channels = 3, embed_dim = 768, patch_size = 16):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.grid_size = img_size // patch_size\n",
        "    self.num_patches = self.grid_size **2\n",
        "    self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.projection(x)\n",
        "    x = x.flatten(2).transpose(1,2)\n",
        "    return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_features, hidden_features=None, out_features=None, dropout=0.1):\n",
        "    super().__init__()\n",
        "    hidden_features = hidden_features or in_features\n",
        "    out_features = out_features or in_features\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(in_features, hidden_features),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_features, out_features),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc(x)\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self, dim, num_heads, mlp_ratio, dropout):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim)\n",
        "    self.atten = nn.MultiheadAttention(embed_dim = dim, num_heads=num_heads, batch_first=True)\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "    self.mlp = MLP(dim, int(dim*mlp_ratio), dropout=dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.atten(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self, img_size=224, patch_size = 16, in_channels = 3, embed_dim = 768, num_heads = 12,\n",
        "               mlp_ratio = 4.0, dropout = 0.1, depth= 12, num_classes = 1000):\n",
        "    super().__init__()\n",
        "    self.patch_embeddings = PatchEmbedding(img_size, in_channels, embed_dim, patch_size)\n",
        "    self.num_patches = self.patch_embeddings.num_patches\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches+1, embed_dim))\n",
        "    self.posDrop = nn.Dropout(p=dropout)\n",
        "\n",
        "    self.blocks = nn.Sequential(*[TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "    for _ in range(depth)])\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    self._init_weights()\n",
        "\n",
        "  def _init_weights(self):\n",
        "    nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "    nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "    nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
        "    if self.head.bias is not None:\n",
        "      nn.init.zeros_(self.head.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B = x.shape[0]\n",
        "    x = self.patch_embeddings(x)\n",
        "    cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_tokens, x), dim = 1)\n",
        "    x = x + self.pos_embed\n",
        "    x = self.posDrop(x)\n",
        "\n",
        "    x = self.blocks(x)\n",
        "    x = self.norm(x)\n",
        "    x = x[:,0]\n",
        "    x = self.head(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "LiwyeqtiBSVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(img_size = 224, patch_size=16, num_classes=10)\n",
        "dummy_input = torch.randn(2, 3, 224, 224)\n",
        "output = model(dummy_input)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlwPHG93Pc8Z",
        "outputId": "fcf497b8-491f-4598-8f1e-eb267e2e1900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vit encoder for object detection\n",
        "import torch.nn.functional as F\n",
        "class DetectionVit(nn.Module):\n",
        "  def __init__(self, img_size=224, patch_size=16, embed_dim=768,\n",
        "               num_heads = 12, depth=6, mlp_ratio=4.0, num_box = 1, num_classes =1, in_channel=3, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.patch_embed = PatchEmbedding(img_size, in_channel, embed_dim, patch_size)\n",
        "    self.num_patches = self.patch_embed.num_patches\n",
        "    self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches+1, embed_dim))\n",
        "\n",
        "    self.blocks = nn.Sequential(*[TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout) for _ in range(depth)])\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.num_box= num_box\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.Linear(embed_dim, embed_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embed_dim, self.num_box*(4+self.num_classes))\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    B = x.shape[0]\n",
        "    x = self.patch_embed(x)\n",
        "    cls_tokens = self.cls_token.expand(B, -1,-1)\n",
        "    x = torch.cat([cls_tokens,x],dim=1)\n",
        "    x = x + self.pos_embed\n",
        "    x = self.blocks(x)\n",
        "    x = self.norm(x)\n",
        "    x = x[:,0]\n",
        "    x = self.mlp_head(x)\n",
        "    return x\n",
        "\n",
        "Detector = DetectionVit()\n",
        "dummy_input = torch.randn(2, 3, 224, 224)\n",
        "output = Detector(dummy_input)\n",
        "print(output.shape)\n",
        "\n",
        "bbox_pred = output[:,:4]\n",
        "cls_pred = output[:,4:]\n",
        "print(bbox_pred.shape)\n",
        "print(cls_pred.shape)\n",
        "\n",
        "def detection_loss(bbox_pre, bbox_gt, cls_pred, cls_gt):\n",
        "  bbox_loss = F.mse_loss(bbox_pre, bbox_gt)\n",
        "  class_loss = F.binary_cross_entropy_with_logits(cls_pred,cls_gt)\n",
        "  return bbox_loss + class_loss\n",
        "\n",
        "target = torch.randn(2,4)\n",
        "target_cls = torch.randn(2,1)\n",
        "loss = detection_loss(bbox_pred, target, cls_pred, target_cls)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9y3n1n88NSm",
        "outputId": "b94e58d9-0ed7-4882-ade0-58778481d838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5])\n",
            "torch.Size([2, 4])\n",
            "torch.Size([2, 1])\n",
            "tensor(1.3038, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vit encoder for segmentation\n",
        "class SegmentVit(nn.Module):\n",
        "  def __init__(self, img_size=224, patch_size=16, embed_dim=768,\n",
        "               num_heads = 12, depth=6, mlp_ratio=4.0, num_box = 1, num_classes =2, in_channel=3, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.patch_embed = PatchEmbedding(img_size, in_channel, embed_dim, patch_size)\n",
        "    self.num_patches = self.patch_embed.num_patches\n",
        "    self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches+1, embed_dim))\n",
        "\n",
        "    self.blocks = nn.Sequential(*[TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout) for _ in range(depth)])\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.num_box= num_box\n",
        "    self.num_classes = num_classes\n",
        "    self.patch_size = patch_size\n",
        "    self.img_size = img_size\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "         nn.Conv2d(embed_dim, 256, kernel_size=3, padding=1),\n",
        "         nn.BatchNorm2d(256),\n",
        "         nn.ReLU(inplace=True),\n",
        "         nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "         nn.ReLU(inplace=True),\n",
        "         nn.Conv2d(128, num_classes, kernel_size=1),\n",
        "         )\n",
        "\n",
        "  def forward(self, x):\n",
        "    B = x.shape[0]\n",
        "    x = self.patch_embed(x)\n",
        "    cls_tokens = self.cls_token.expand(B, -1,-1)\n",
        "    x = torch.cat([cls_tokens,x],dim=1)\n",
        "    x = x + self.pos_embed\n",
        "    x = self.blocks(x)\n",
        "    x = self.norm(x)\n",
        "    x = x[:,1:, :] #B, N, D\n",
        "    b, n, d = x.shape\n",
        "    x = x.permute(0,2,1).reshape(B, -1 ,int(n**0.5),int(n**0.5)) #assuming square\n",
        "    x = F.interpolate(x, size=(self.img_size, self.img_size), mode = 'bilinear', align_corners=True)\n",
        "    x = self.mlp_head(x)\n",
        "    return x\n",
        "\n",
        "Segmentor = SegmentVit()\n",
        "dummy_input = torch.randn(2, 3, 224, 224)\n",
        "output = Segmentor(dummy_input)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZzjd3a-AVHw",
        "outputId": "d8d310fc-5643-4b49-dc5d-7014b032b0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD pretrained ViT for classification on CIFER-10"
      ],
      "metadata": {
        "id": "kGIgU6QYh3Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import timm\n",
        "\n",
        "#Load Model\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained = True)\n",
        "#print(model)\n",
        "\n",
        "# Modify Classifier\n",
        "model.head = nn.Linear(model.head.in_features, 10)\n",
        "\n",
        "#Freeze Model except classifier\n",
        "for name, param in model.named_parameters():\n",
        "  if not name.startswith(\"head\"):\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "#Dataset Handeling\n",
        "transform = transforms.Compose([transforms.Resize(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root = './data', train = True, download=True, transform=transform)\n",
        "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle= False)\n",
        "\n",
        "\n",
        "#Training Pipeline\n",
        "device = torch.device(\"cuda \" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss\n",
        "    correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "  train_loss = total_loss / len(train_loader)\n",
        "  train_acc = correct / len(train_dataset)\n",
        "\n",
        "  print(f\"Epoch: {epoch+1} Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dlH9b_YD0Tl",
        "outputId": "761475fe-41d9-4ae7-8252-abfab9230f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 73.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "\n",
        "for images, target in val_loader:\n",
        "  images, target = images.to(device), target.to(device)\n",
        "  output = model(images)\n",
        "  correct += (output.argmax(1) == target).sum().item()\n",
        "\n",
        "val_acc = correct / len(val_dataset)\n",
        "\n",
        "print(f\"Accuracy: {100. * val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "K6xjHoFcCvO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "USER DEFINE CODE FOR MHA BLOCK"
      ],
      "metadata": {
        "id": "qlretPFCZC5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_features),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_features, out_features),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // num_heads\n",
        "        assert self.head_dim * num_heads == dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "\n",
        "        # Learnable Q, K, V projections\n",
        "        self.q_proj = nn.Linear(dim, dim)\n",
        "        self.k_proj = nn.Linear(dim, dim)\n",
        "        self.v_proj = nn.Linear(dim, dim)\n",
        "\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = self.norm1(x)\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.q_proj(x_norm)\n",
        "        k = self.k_proj(x_norm)\n",
        "        v = self.v_proj(x_norm)\n",
        "\n",
        "        # Reshape for multi-head\n",
        "        q = q.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, N, head_dim)\n",
        "        k = k.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, num_heads, N, N)\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        attn_output = attn_probs @ v  # (B, num_heads, N, head_dim)\n",
        "\n",
        "        # Concatenate heads\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(B, N, D)  # (B, N, D)\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "\n",
        "        # Residual + Feed Forward\n",
        "        x = x + attn_output\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "koowjsPMh2Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self, dim, num_heads, mlp_ratio):\n",
        "    super().__init__()\n",
        "\n",
        "    self.head_dim = dim // num_heads\n",
        "    assert self.head_dim * num_heads == dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(dim)\n",
        "    #self.atten = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
        "    self.q_proj = nn.Linear(dim,dim)\n",
        "    self.k_proj = nn.Linear(dim,dim)\n",
        "    self.v_proj = nn.Linear(dim,dim)\n",
        "\n",
        "    self.out_proj = nn.Linear(dim,dim)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(dim, int(dim * mlp_ratio)),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(int(dim * mlp_ratio), dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N, D = x.shape\n",
        "    x_norm = self.norm1(x)\n",
        "\n",
        "    q = self.q_proj(x_norm)\n",
        "    k = self.k_proj(x_norm)\n",
        "    v = self.v_proj(x_norm)\n",
        "\n",
        "    q = q.view(B, N, self.num_heads, self.head_dim).transpose(1,2)\n",
        "    k = k.view(B, N, self.num_heads, self.head_dim).transpose(1,2)\n",
        "    v = v.view(B, N, self.num_heads, self.head_dim).transpose(1,2)\n",
        "\n",
        "    atten_scores = (q @ k.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
        "    atten_probs = F.softmax(atten_scores, dim=-1)\n",
        "    atten_probs = self.dropout(atten_probs)\n",
        "    atten_out = atten_probs @ v\n",
        "\n",
        "    atten_out = atten_out.transpose(1,2).contiguous().view(B, N, D)\n",
        "    atten_out = self.out_proj(atten_out)\n",
        "    atten_out = self.dropout(atten_out)\n",
        "\n",
        "    x = x + atten_out\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x\n",
        "\n",
        "TEB = TransformerEncoderBlock(dim=128, num_heads=8, mlp_ratio=4)\n",
        "x = torch.randn(1, 10, 128)\n",
        "y = TEB(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "5S9XToo6zage"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zTF2qynbZSAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD CLIP TO EXTRACT TEXT EMBEDDINGS"
      ],
      "metadata": {
        "id": "GsRSzj6HZS4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q open_clip_torch transformers\n",
        "# !pip install -q ftfy\n",
        "\n",
        "import open_clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "\n",
        "# Replace 'Your text here' with the text you want to get embeddings for\n",
        "text = tokenizer([\"a dog\", \"a cat\", \"a horse\"])\n",
        "text_features = model.encode_text(text)\n",
        "\n",
        "text_features.shape"
      ],
      "metadata": {
        "id": "A2-bFtVAWX1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}